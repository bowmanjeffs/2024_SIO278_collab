---
title: "BLE_Microbio_18s"
author: "Zach Brown"
date: "2024-02-22"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#housekeeping code and calling things in
library(readr)
library(tidyverse)
library(ggplot2)
library(readxl)

library(lubridate)
library(vegan)
library(plotly) #couldn't load
library(goeveg) #couldn't load
library(patchwork)

#######required libraries#######################
library(randomForest)
library(caret)
library(e1071)
library(dplyr)

library(tidyverse)
library(ranger)
library(lubridate)
library(vegan)
library(ranger)
library(Boruta)


setwd("/Users/zachbrown/Desktop/SIO/Classes/2023-24/Winter2024/SIOB278_MarineMicrobialSeminar/BLE_LTER_Data")

```


#Attempt 1 RF - regression

```{r}
#Should we divide up by filter size as well?
#Start with 16s data, RF_classification template
library(randomForest)
library(caret)
library(e1071)
library(dplyr)
```

Work with MergedData16sSalinity3 for RF.  This chunk discusses how I tailored the dataset to meet the RF requirements, starting from confirming numeric data to converting to relative abundance to pulling the ASV and one env data predicted value (salinity) and making sure the data frame was of an okay size.

```{r Prep data and run RF - step 1}

#import main .csv - ~ 18s ASV values and all env data - how to get data ready to go from env/ASV .csv
EnvASVData18 <- read.csv("EnvASV18Data2.csv")
EnvASVData18_2 <- t(EnvASVData18)
colnames(EnvASVData18_2) <- EnvASVData18_2[1, ]
EnvASVData18_2 <- EnvASVData18_2[-1, ]
EnvASVData18_2 <- as.data.frame(EnvASVData18_2)

Salinity18ASVData <- EnvASVData18_2[, c(8, 24:14145), drop = FALSE]
Salinity18ASVData2 <- Salinity18ASVData[!is.na(Salinity18ASVData[, 'Salinity']), ]
str(Salinity18ASVData2)
Salinity18ASVData3 <- apply(Salinity18ASVData2, MARGIN = 2, FUN = as.numeric)
Salinity18ASVData3 <- as.data.frame(Salinity18ASVData3)
str(Salinity18ASVData3)

#Set a dataframe as the dataframe we want to put through random forest
Data18sTry1 <- Salinity18ASVData3

# Data Partition in 70% training dataset and 30% test dataset - directly from code
set.seed(123)
ind <- sample(2, nrow(Data18sTry1), replace = TRUE, prob = c(0.7, 0.3))
train18sSalinity <- Data18sTry1[ind==1,]
test18sSalinity <- Data18sTry1[ind==2,]

#running rf - taken directly from code
set.seed(4444)
rf18sSalinity1 <- randomForest(Salinity~., #looking at phase while compared to all the columns so dot (.) is given after tilda (~)
                   data=train18sSalinity, #train data is used to train
                   ntree = 200, #number of trees to run
                   importance = TRUE, #evaluates importance of a predictor
                   proximity = TRUE) #calculates the proximity measure among the rows

#plotting error-shows the error with growing number of trees in a forest
plot(rf18sSalinity1)

#saving the parameters used in running RF
sink(paste0("RF18Sal_parameter",".txt"))
rf18sSalinity1
sink()

#predicting the dependent variable in the training set using generated RF model
p1 <- predict(rf18sSalinity1, train18sSalinity)
write.csv(p1,paste0("trainingSalinity18s",".csv"))

#predicting the dependent variable in the validation set using generated RF model
p2 <- predict(rf18sSalinity1, test18sSalinity)
write.csv(p2,paste0("validationSalinity18s",".csv"))

```


```{r Training Accuracy - step 2}
#read in data
data18 <- Salinity18ASVData3 #define the dataframe we used for training
data18_2 <- read.csv(paste0("trainingSalinity18s",".csv")) #pull in data from RF
colnames(data18_2) <- c("SampleNumber", "ActualValue") #and rename it

#pull real data for samples that line up with the training dataset
df2 <- data18 %>%  select(1, Salinity) #pull all salinity values
df2 <- data.frame(blank_column = NA, df2) #create a blank column aligning with salinity values
df2$blank_column <- 1:222 #label blank column with the number of rows there are 
colnames(df2) <- c("SampleNumber", "ActualValue") #change the column names

#call in the the trained data with the predicted values

tr18 <- read.csv(paste0("trainingSalinity18s",".csv")) #call in the csv saved at the end of the last chunk
colnames(tr18) <- c("SampleNumber", "PredictedValue") #rename the column names
trainingdatapoints18 <- tr18[,1] #make a vector that has the salinity values that were predicted in the above random forest model

#From the training data, pull the associated 
indices18 <- match(trainingdatapoints18, df2$SampleNumber) #make a list of the row numbers that match with the trained data
df2_18 <- df2[indices18, ] #make a new dataframe that pulls the correct rows out of the dataframe that had all of the real data
colnames(df2_18) <- c("SampleNumber", "ActualValue")

#Finally merge the training data and the real values into a new dataframe by the sample number
total18 <- merge(tr18,df2_18,by="SampleNumber")
write.csv(total18,"predicted_vs_actual_in_training18Sal.csv", row.names = FALSE) #save the dataframe as a CSV


#plotting predicted vs actual value

#pdf(paste0("Training_plot",".pdf"),8,4) - don't need to run this I don't think
ggplot(total18, aes(y=ActualValue, x=PredictedValue)) +  geom_point() + geom_smooth(method=lm)+
  ggtitle(paste0("Model RM"," training plot")) +
  xlab("Predicted Salinity") + ylab("Actual Salinity") 
#dev.off() - also don't need to run this

#running linear model for training - did not change any of this from the example
fit <- lm(ActualValue ~ PredictedValue, data = total18)
sink(paste0("training","_accuracy18.txt"))
print(summary(fit))
sink()
```

```{r}
ggplot(x=train18sSalinity$Salinity, y=p1)
train18sSalinity$PredictedSalinity<-p1
ggplot(data=train18sSalinity, aes(x=Salinity, y=PredictedSalinity)) +
  geom_point()+
  geom_smooth(method = "lm")+
  xlab("Actual Salinity Concentration")+
  ylab("Predicted Salinity Concentration")+
  ggtitle("Random Forest Training Data Validation")

Model18_1<-lm(Salinity~PredictedSalinity,data=train18sSalinity)
summary(Model18_1) #Same as what was put out by the linear model for training
```


```{r Validation Accuracy - Step 3}

pr18 <- read.csv(paste0("validationSalinity18s",".csv"))
colnames(pr18) <- c("SampleNumber", "PredictedValue")
validationdatapoints18 <- pr18[,1] #make a vector that has the salinity values that were predicted in the above random forest model
#From the training data, pull the associated 
indices2_18 <- match(validationdatapoints18, df2$SampleNumber) #make a list of the row numbers that match with the trained data
df2_18_2 <- df2[indices2_18, ] #make a new dataframe that pulls the correct rows out of the dataframe that had all of the real data
colnames(df2_18_2) <- c("SampleNumber", "ActualValue")

total2_18 <- merge(pr18,df2_18_2,by="SampleNumber")
write.csv(total2_18,"predicted_vs_actual_in_validation18.csv", row.names = FALSE)

#plotting predicted vs actual value

#pdf(paste0("Validation_plot",x,y,z,".pdf"),8,4)
ggplot(total2_18, aes(y=ActualValue, x=PredictedValue)) +  geom_point() + geom_smooth(method=lm)+  
  ggtitle(paste0("Model RM"," validation plot")) +
  xlab("Predicted Salinity") + ylab("Actual Salinity") 
#dev.off()

#running linear model for validation
fit2_18 <- lm(ActualValue ~ PredictedValue, data = total2_18)

sink(paste0("validation","_accuracy18.txt"))
print(summary(fit2_18))
sink()
```


```{r}
ggplot(x=test16sSalinity$Salinity, y=p2)
test16sSalinity$PredictedSalinity<-p2
ggplot(data=train16sSalinity, aes(x=Salinity, y=PredictedSalinity)) +
  geom_point()+
  geom_smooth(method = "lm")+
  xlab("Actual Salinity Concentration")+
  ylab("Predicted Salinity Concentration")+
  ggtitle("Random Forest Training Data Validation")

Model2<-lm(Salinity~PredictedSalinity,data=test16sSalinity)
summary(Model2)
```


```{r}
#Follow example from microbiome group

#predictors should be all ASV values used in RF model
predictors <- Salinity18ASVData3[ , c(2:ncol(Salinity18ASVData3))]

hyper.grid <- expand.grid(
  n.edges = seq(100, 2000, 100), # aka n.trees
  mtry       = seq(10, 30, by = 2), # 
  node_size  = seq(3, 9, by = 2), 
  sample_size = c(.55, .632, .70), # internal
  OOB_RMSE   = 0 # internal
)

for(i in 1:nrow(hyper.grid)){ ## AKA for every combination of parameter settings
  
  # predictors <- boruta.index[order(colSums(asv.train)[colnames(asv.train) %in% boruta.index], decreasing = T)] # cannot use more n.edges than boruta predictors
  
  try({ ## try clause necessary because some parameter combinations are incompatible
    
    model <- ranger(
      formula = Salinity~.,
      data = train18sSalinity, 
      num.trees       = hyper.grid$n.edges[i],
      mtry            = hyper.grid$mtry[i],
      min.node.size   = hyper.grid$node_size[i],
      sample.fraction = hyper.grid$sample_size[i],
      seed            = 123,
    )
    
    ## add OOB error to grid
    hyper.grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
    
    ## From the internet: 
    ## OOB (out-of-bag) score is a performance metric for a machine learning model, 
    ## specifically for ensemble models such as random forests. 
    ## It is calculated using the samples that are not used in the training of the model, 
    ## which is called out-of-bag samples.
    ## The OOB_score is computed as the number of correctly predicted rows from the out-of-bag sample. 
    ## OOB Error is the number of wrongly classifying the OOB Sample.
    
  }, silent = F)
  
  #print(paste(i, 'out of', nrow(hyper.grid), hyper.grid$OOB_RMSE[i]))
  
}

hyper.grid$OOB_RMSE[hyper.grid$OOB_RMSE == 0] <- NA
hyper.grid <- na.omit(hyper.grid)

hist(hyper.grid$OOB_RMSE, breaks = 100)

## define selected optimal parameters for the model
selected.params <- hyper.grid[which.min(hyper.grid$OOB_RMSE),]
```

